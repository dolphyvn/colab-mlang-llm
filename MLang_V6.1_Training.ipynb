{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLang V6.1 Training - Google Colab\n",
    "\n",
    "This notebook trains a model on MLang V6.1 data (lookahead-based prediction).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Generate training data locally using:\n",
    "   ```bash\n",
    "   python encode_mlang_v6_full.py --input-dir ./data --output-txt mlang_v6/train.txt --output-meta mlang_v6/meta.json --seq-len 12 --lookahead 4 --feature-level top1\n",
    "   ```\n",
    "\n",
    "2. Convert to ChatML format:\n",
    "   ```bash\n",
    "   python convert_mlang_v6_to_chatml.py mlang_v6/train.txt mlang_v6/train.jsonl\n",
    "   python convert_mlang_v6_to_chatml.py mlang_v6/val.txt mlang_v6/val.jsonl\n",
    "   ```\n",
    "\n",
    "3. Upload `train.jsonl` and `val.jsonl` to Colab\n",
    "\n",
    "## V6.1 Format\n",
    "- Input: `seq_len` context candles (without OUTCOME)\n",
    "- Output: `lookahead` OUTCOME tokens (predictions for next N candles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate peft trl bitsandbytes\n",
    "!pip install -q sentencepiece protobuf\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import login\n\n# Option 1: Enter your token when prompted (interactive)\n# login()\n\n# Option 2: Paste your token directly (replace YOUR_TOKEN_HERE below)\nlogin(token=\"YOUR_TOKEN_HERE\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. HuggingFace Authentication\n\nRequired for gated models like Llama 3.2. \n\nGet your token at: https://huggingface.co/settings/tokens\n\nYou also need to accept the Llama 3.2 license at: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload `train.jsonl` and `val.jsonl` files using the file browser on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploaded files\n",
    "import os\n",
    "\n",
    "train_file = \"/content/drive/Shareddrives/D1/Data/train.jsonl\"\n",
    "val_file = \"/content/drive/Shareddrives/D1/Data/val.jsonl\"\n",
    "\n",
    "if os.path.exists(train_file):\n",
    "    with open(train_file) as f:\n",
    "        train_lines = sum(1 for _ in f)\n",
    "    print(f\"✓ Train file: {train_lines} sequences\")\n",
    "else:\n",
    "    print(\"✗ Train file not found! Please upload train.jsonl\")\n",
    "\n",
    "if os.path.exists(val_file):\n",
    "    with open(val_file) as f:\n",
    "        val_lines = sum(1 for _ in f)\n",
    "    print(f\"✓ Val file: {val_lines} sequences\")\n",
    "else:\n",
    "    print(\"✗ Val file not found! Please upload val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"  # or \"google/gemma-3-270m-it\" for faster training\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_SEQ_LENGTH = 2048  # Reduce if OOM\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"/content/mlang_v6_model\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} × {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} effective\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max seq length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  LoRA r: {LORA_R}, alpha: {LORA_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom transformers import TrainerCallback\n\n\nclass PredictionCallback(TrainerCallback):\n    \"\"\"Callback to show sample predictions during training.\"\"\"\n    \n    def __init__(self, tokenizer, val_dataset, num_samples=2, max_new_tokens=50):\n        self.tokenizer = tokenizer\n        self.val_dataset = val_dataset\n        self.num_samples = num_samples\n        self.max_new_tokens = max_new_tokens\n    \n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 100 == 0 and state.global_step > 0:\n            model = kwargs.get('model')\n            if model is None:\n                return\n            \n            print(f\"\\n{'='*60}\")\n            print(f\"Step {state.global_step} - Sample Predictions:\")\n            \n            for i in range(min(self.num_samples, len(self.val_dataset))):\n                sample = self.val_dataset[i]\n                text = sample['text']\n                \n                # Find the assistant prompt\n                before_assistant = text.split('<|im_start|>assistant')[0]\n                prompt = before_assistant + '<|im_start|>assistant\\n'\n                \n                # Expected output\n                if '<|im_start|>assistant' in text:\n                    expected = text.split('<|im_start|>assistant')[1].split('<|im_end|>')[0].strip()\n                else:\n                    expected = \"N/A\"\n                \n                inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n                if inputs.shape[1] > args.max_seq_length - self.max_new_tokens:\n                    inputs = inputs[:, -(args.max_seq_length - self.max_new_tokens):]\n                \n                inputs = inputs.to(model.device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        inputs,\n                        max_new_tokens=self.max_new_tokens,\n                        do_sample=False,\n                        pad_token_id=self.tokenizer.pad_token_id,\n                        eos_token_id=self.tokenizer.eos_token_id\n                    )\n                \n                predicted = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n                \n                # Calculate match percentage\n                expected_tokens = expected.split()\n                predicted_tokens = predicted.split()\n                matches = sum(1 for e, p in zip(expected_tokens, predicted_tokens) if e == p)\n                match_pct = (matches / len(expected_tokens) * 100) if expected_tokens else 0\n                \n                print(f\"  Sample {i+1}:\")\n                print(f\"    Expected: {expected[:100]}...\")\n                print(f\"    Predict: {predicted[:100]}...\")\n                print(f\"    Match: {match_pct:.1f}%\")\n            print(f\"{'='*60}\\n\")\n\n\ndef load_jsonl(file_path):\n    \"\"\"Load JSONL file into list of dicts.\"\"\"\n    data = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data\n\n\ndef main():\n    # Load data\n    print(\"Loading training data...\")\n    train_data = load_jsonl(train_file)\n    val_data = load_jsonl(val_file)\n    print(f\"  Train: {len(train_data)} sequences\")\n    print(f\"  Val: {len(val_data)} sequences\")\n    \n    # Create datasets\n    train_dataset = Dataset.from_list(train_data)\n    val_dataset = Dataset.from_list(val_data)\n    \n    # Load tokenizer\n    print(f\"\\nLoading tokenizer: {BASE_MODEL}\")\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = 'right'\n    \n    # Load model\n    print(f\"Loading model: {BASE_MODEL}\")\n    model = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    model.config.use_cache = False\n    \n    # Configure LoRA\n    print(\"\\nConfiguring LoRA...\")\n    lora_config = LoraConfig(\n        r=LORA_R,\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False\n    )\n    model = get_peft_model(model, lora_config)\n    print(f\"  Trainable params: {model.get_nb_trainable_parameters()}\")\n    \n    # Tokenize function with proper label masking\n    def tokenize_function(examples):\n        \"\"\"Tokenize and create labels that only compute loss on assistant responses.\"\"\"\n        # Tokenize the full text\n        full_tokens = tokenizer(\n            examples['text'],\n            max_length=MAX_SEQ_LENGTH,\n            truncation=True,\n            padding=False,\n            return_tensors=None\n        )\n\n        # For each example, mask the instruction part (system + user)\n        # Only compute loss on the assistant's response\n        labels_list = []\n        for i, text in enumerate(examples['text']):\n            input_ids = full_tokens['input_ids'][i]\n\n            # Find where the assistant response starts\n            if '<|im_start|>assistant' in text:\n                # Get the instruction part (everything before assistant response)\n                before_assistant = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant\\n'\n\n                # Tokenize just the instruction part to get its length\n                instruction_tokens = tokenizer(\n                    before_assistant,\n                    max_length=MAX_SEQ_LENGTH,\n                    truncation=True,\n                    padding=False,\n                    add_special_tokens=False  # Don't add special tokens again\n                )\n\n                instruction_length = len(instruction_tokens['input_ids'])\n\n                # Create labels: -100 for instruction tokens, actual IDs for response\n                example_labels = [-100] * instruction_length + input_ids[instruction_length:]\n\n                # Ensure labels match input_ids length\n                example_labels = example_labels[:len(input_ids)]\n            else:\n                # If no assistant tag found (shouldn't happen), mask everything\n                print(f\"Warning: No assistant tag found in example {i}\")\n                example_labels = [-100] * len(input_ids)\n\n            labels_list.append(example_labels)\n\n        full_tokens['labels'] = labels_list\n        return full_tokens\n    \n    # Tokenize datasets\n    print(\"\\nTokenizing datasets...\")\n    train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n    val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n    \n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n        pad_to_multiple_of=8\n    )\n    \n    # Training arguments (removed evaluation_strategy for compatibility)\n    training_args = TrainingArguments(\n        output_dir=OUTPUT_DIR,\n        num_train_epochs=NUM_EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=10,\n        save_steps=100,\n        save_total_limit=2,\n        report_to=\"none\",\n        save_strategy=\"steps\",\n        max_grad_norm=1.0\n    )\n    \n    # Create trainer with prediction callback\n    prediction_callback = PredictionCallback(tokenizer, val_dataset, num_samples=2)\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=data_collator,\n        callbacks=[prediction_callback]\n    )\n    \n    # Train\n    print(\"\\n\" + \"=\"*60)\n    print(\"Starting training...\")\n    print(\"=\"*60 + \"\\n\")\n    \n    trainer.train()\n    \n    # Save final model\n    print(\"\\nSaving final model...\")\n    trainer.save_model(OUTPUT_DIR)\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    print(f\"Model saved to: {OUTPUT_DIR}\")\n    \n    return trainer\n\n# Run training\ntrainer = main()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Model\n",
    "\n",
    "After training, download the model files from the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List model files\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        size = os.path.getsize(filepath) / 1024 / 1024  # MB\n",
    "        print(f\"{file}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easy download\n",
    "!zip -r /content/mlang_v6_model.zip /content/mlang_v6_model\n",
    "print(\"\\nDownload mlang_v6_model.zip from the file browser\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
